{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Robust Membership Inference Attack Tutorial","metadata":{"_uuid":"0fc57f82-7c6a-4b1b-bdc0-e75fd6df0fb6","_cell_guid":"78fba96d-5217-443c-a71b-bf6bcff4f46f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%matplotlib inline  \n\nimport random\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim.lr_scheduler import StepLR\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom tqdm import tqdm\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n# Ensure reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"_uuid":"2c21a793-97a3-44ed-ba8a-a78657d05e38","_cell_guid":"a6ca5568-103e-4fb2-8fa7-97cd11626466","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Setup (Challenger Model Dataset and Reference Model Data Split)","metadata":{"_uuid":"81a76f26-a200-4dbd-b366-17204570fe22","_cell_guid":"a1af04aa-68a5-4f1b-9fe6-06ddb8e3c685","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Dataset Preparation\ntransform = transforms.Compose([\n     transforms.ToTensor(),\n     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrain_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\n# Split into training and shadow sets\ntrain_size = int(0.5 * len(train_data))\nshadow_size = len(train_data) - train_size\ntrain_set, shadow_set = random_split(train_data, [train_size, shadow_size])\n\nnum_classes = 10","metadata":{"_uuid":"24311530-241c-45b9-918f-999f7048f46e","_cell_guid":"24be9c58-5648-4827-9bb9-fe273550b05c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Architecture: ResNet18\ndef get_resnet_model(num_classes):\n    model = models.resnet18(pretrained=True)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model","metadata":{"_uuid":"1173e6f2-eec4-44fa-857f-cf5b4a81320c","_cell_guid":"83427fe3-2262-4432-8a89-75655ba06ade","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(get_resnet_model(10))","metadata":{"_uuid":"c87156b1-2004-4b84-bc75-31e314262b4e","_cell_guid":"b177ab2e-6216-4abb-9486-028a77def6d4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training/Loading Challenger Model","metadata":{"_uuid":"efee772a-1802-40fc-a259-ffa94a746f32","_cell_guid":"b586db95-808d-44dc-ac90-fd62848baa51","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Training Parameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nepochs = 25\nbatch_size = 256\nlearning_rate = 0.1\nweight_decay = 5e-4\nmomentum = 0.9\nnum_classes = 10\ncheckpoint_dir = \"./checkpoints_new4\"  # Directory to save/load checkpoints\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Function to save checkpoint\ndef save_checkpoint(model, optimizer, epoch, path):\n    checkpoint = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch\n    }\n    torch.save(checkpoint, path)\n    print(f\"Checkpoint saved at {path}\")\n\n# Function to load checkpoint\ndef load_checkpoint(model, optimizer, path):\n    if os.path.isfile(path):\n        checkpoint = torch.load(path)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n        # Move the model and optimizer states to the appropriate device\n        model = model.to(device)\n        for state in optimizer.state.values():\n            if isinstance(state, torch.Tensor):\n                state.data = state.data.to(device)\n            elif isinstance(state, dict):\n                for k, v in state.items():\n                    if isinstance(v, torch.Tensor):\n                        state[k] = v.to(device)\n\n        start_epoch = checkpoint[\"epoch\"]\n        print(f\"Checkpoint loaded from {path}, starting at epoch {start_epoch + 1}\")\n        return model, optimizer, start_epoch\n    else:\n        print(f\"No checkpoint found at {path}, starting fresh.\")\n        return model, optimizer, 0\n\n# Training Function\ndef train_model(model, train_loader, optimizer, criterion, epochs, test_loader=None, checkpoint_path=None):\n    start_epoch = 0\n\n    # Load from checkpoint if provided\n    if checkpoint_path:\n        model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n\n    # Ensure model is on the correct device\n    model.to(device)\n\n    for epoch in range(start_epoch, epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=True)\n\n        for images, labels in loop:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Statistics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            # Update tqdm description\n            loop.set_postfix(loss=loss.item(), accuracy=100. * correct / total)\n\n        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {running_loss / len(train_loader):.4f}, \"\n              f\"Train Accuracy: {100. * correct / total:.2f}%\")\n\n        # Save checkpoint at the end of each epoch\n        if checkpoint_path:\n            save_checkpoint(model, optimizer, epoch, checkpoint_path)\n\n        # Evaluate on test set every 5 epochs\n        if test_loader and (epoch + 1) % 5 == 0:\n            evaluate_model(model, test_loader, epoch + 1)\n\n    return model\n\n# Evaluation Function\ndef evaluate_model(model, test_loader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        loop = tqdm(test_loader, desc=f\"Evaluating at Epoch {epoch}\", leave=True)\n        for images, labels in loop:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    accuracy = 100. * correct / total\n    precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n    recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n\n    print(f\"Test Results at Epoch {epoch} - Accuracy: {accuracy:.2f}%, \"\n          f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")","metadata":{"_uuid":"eadd20cb-12a6-4c46-8049-bdb203671915","_cell_guid":"bf5d0c14-b8bd-4216-ad97-b4342e57af97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Training Process for Target Model\nprint(\"Training Target Model...\")\ntarget_model = get_resnet_model(num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(target_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\ntarget_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n# Checkpoint path for the target model\ntarget_checkpoint_path = os.path.join(checkpoint_dir, \"target_model.pth\")\n\ntarget_model = train_model(target_model, target_loader, optimizer, criterion, epochs, test_loader, target_checkpoint_path)","metadata":{"_uuid":"39d3f01d-20ac-43bc-ba87-2542bffdb46b","_cell_guid":"b6f4bfd0-fd92-417d-acd9-a2e2bcff3959","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Robust MIA Setup","metadata":{"_uuid":"ea2cea46-2f3d-493b-8395-b7909e6d1ba5","_cell_guid":"a7facc66-0125-4855-b464-4b5bdb264743","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Parameters\nnum_reference_models = 5  # Number of reference models\nmax_usage_ratio = 0.1  # Maximum usage of 10% of the shadow dataset\nbatch_size = 128  # Batch size for DataLoader\nattack_mode = \"offline\"  # \"offline\" or \"online\"\nseed = 42  # Random seed for reproducibility\n\n# Set random seed\ntorch.manual_seed(seed)\n\n# Function to generate reference datasets with max usage constraint\ndef generate_limited_reference_datasets(shadow_set, num_models, max_usage_ratio, include_target=False):\n    \"\"\"\n    Generate reference datasets while limiting the total dataset usage.\n    \n    Args:\n        shadow_set: The shadow dataset from which reference datasets are sampled.\n        num_models: Number of reference models.\n        max_usage_ratio: Maximum fraction of the dataset to use across all models.\n        include_target: Whether to include target data in some models (for online attack).\n    \n    Returns:\n        reference_datasets: List of datasets for reference models.\n    \"\"\"\n    total_allowed_samples = int(max_usage_ratio * len(shadow_set))\n    samples_per_model = total_allowed_samples // num_models  # Evenly distribute samples across models\n    \n    shadow_indices = list(range(len(shadow_set)))\n    reference_datasets = []\n    \n    for i in range(num_models):\n        if include_target and i % 2 == 0:  # IN models (only for online attack)\n            subset_indices = random.sample(shadow_indices, samples_per_model)\n        else:  # OUT models\n            subset_indices = random.sample(shadow_indices, samples_per_model)\n        \n        reference_datasets.append(Subset(shadow_set, subset_indices))\n    \n    return reference_datasets\n\n# Generate reference datasets\nif attack_mode == \"online\":\n    # Online attack: Generate both IN and OUT datasets\n    reference_datasets = generate_limited_reference_datasets(\n        shadow_set, num_reference_models, max_usage_ratio, include_target=True\n    )\nelse:\n    # Offline attack: Generate only OUT datasets\n    reference_datasets = generate_limited_reference_datasets(\n        shadow_set, num_reference_models, max_usage_ratio, include_target=False\n    )","metadata":{"_uuid":"21a5340f-2f62-4cd1-b0a3-1ad702675f46","_cell_guid":"86d0d857-7366-4b65-ac63-a9c80a5c8779","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Reference Models\nreference_model_epochs = 10\n\nprint(\"\\nTraining Reference Models...\")\nreference_models = []\nfor i, ref_dataset in enumerate(reference_datasets):\n    print(f\"\\nTraining Reference Model {i + 1}/{len(reference_datasets)}...\")\n    ref_loader = DataLoader(ref_dataset, batch_size=batch_size, shuffle=True)\n    ref_model = get_resnet_model(num_classes)\n    ref_optimizer = optim.SGD(ref_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n    ref_checkpoint_path = os.path.join(checkpoint_dir, f\"reference_model_{i + 1}.pth\")\n    ref_model = train_model(ref_model, ref_loader, ref_optimizer, criterion, reference_model_epochs, None, ref_checkpoint_path)\n    reference_models.append(ref_model)\n\nprint(\"Training Complete.\")","metadata":{"_uuid":"f41d7a14-9aa3-4d1e-8c0b-77146092a1cf","_cell_guid":"80c4f554-e48d-4838-88cd-7d9c96aa3156","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset\nimport random\n\n# Prepare member and non-member datasets\nref_datapoint_count = 1000  # Total points to sample for members/non-members\nmember_indices = random.sample(range(len(train_set)), ref_datapoint_count // 2)\nnon_member_indices = random.sample(range(len(test_data)), ref_datapoint_count // 2)\n\nmember_dataset = Subset(train_set, member_indices)\nnon_member_dataset = Subset(test_data, non_member_indices)\n\n# DataLoaders for members and non-members\nmember_loader = DataLoader(member_dataset, batch_size=128, shuffle=True)\nnon_member_loader = DataLoader(non_member_dataset, batch_size=128, shuffle=True)","metadata":{"_uuid":"3eaf1a61-6af6-4a1d-b0cf-0d10f9400383","_cell_guid":"8786a534-3e9f-4bd8-a633-1123a329a03c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Robust Membership Inference Attack","metadata":{"_uuid":"65d5d075-34bc-457b-b2ae-ab73be865949","_cell_guid":"dca51e49-e415-4e37-aedf-f0e8bbedc57a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, Subset\nimport torch.nn.functional as F\n\n\n# def compute_P_x(reference_models, x, non_member_loader):\n#     \"\"\"\n#     Compute P(x) using OUT reference models trained on non-members.\n\n#     Args:\n#         reference_models: List of OUT models (trained on non-members).\n#         x: Target data point (dictionary with 'input' and 'label').\n#         non_member_loader: DataLoader providing access to non-member data.\n    \n#     Returns:\n#         P_x: Average probability of x in the population (non-member).\n#     \"\"\"\n#     x_softmax_scores = []\n\n#     for model in reference_models:\n#         model.eval()\n#         with torch.no_grad():\n#             for inputs, _ in non_member_loader:\n#                 inputs = inputs.to(next(model.parameters()).device)\n#                 outputs = model(inputs)\n#                 softmax_scores = F.softmax(outputs, dim=1)\n#                 x_scores = softmax_scores[:, x['label']].cpu().numpy()  # Extract probabilities for x's label\n#                 x_softmax_scores.extend(x_scores)\n\n#     P_x = np.mean(x_softmax_scores)\n#     return P_x\n\n# P(x): Probability of x in the population\ndef compute_P_x(reference_models, x, non_member_loader):\n    \"\"\"\n    Compute P(x) using OUT reference models trained on non-members.\n\n    Args:\n        reference_models: List of OUT models (trained on non-members).\n        x: Target data point (dictionary with 'input' and 'label').\n        non_member_loader: DataLoader providing access to non-member data.\n\n    Returns:\n        P_x: Average probability of x in the population (non-member).\n    \"\"\"\n    x_softmax_scores = []\n\n    for model in reference_models:\n        model.eval()\n        with torch.no_grad():\n            x_input = x[\"input\"].to(next(model.parameters()).device)  # Ensure input is on the same device as the model\n            outputs = model(x_input)\n            softmax_scores = F.softmax(outputs, dim=1)\n            x_scores = softmax_scores[0, x['label']].item()  # Extract the probability for the specific class label\n            x_softmax_scores.append(x_scores)\n\n    P_x = np.mean(x_softmax_scores)\n    return P_x\n\n\n\n# P(Model | x): Likelihood of the model given x\ndef compute_P_Model_given_x(in_reference_models, x):\n    \"\"\"\n    Compute P(Model | x) using IN reference models (trained with x included).\n\n    Args:\n        in_reference_models: List of IN reference models (trained with x).\n        x: Target data point (dictionary with 'input' and 'label').\n\n    Returns:\n        P_Model_given_x: The likelihood of observing the model given x.\n    \"\"\"\n    # Aggregate softmax scores from IN reference models\n    in_softmax_scores = []\n\n    for model in in_reference_models:\n        model.eval()\n        with torch.no_grad():\n            x_input = x[\"input\"].unsqueeze(0).to(next(model.parameters()).device)  # Add batch dimension\n            outputs = model(x_input)\n            softmax_scores = F.softmax(outputs, dim=1)\n            in_softmax_scores.append(softmax_scores[0, x[\"label\"]].item())  # Probability for the true label\n\n    # Compute the mean probability across all IN reference models\n    P_Model_given_x = np.mean(in_softmax_scores)\n    return P_Model_given_x\n\n\n# Likelihood Ratio Computation\ndef compute_likelihood_ratio(reference_models, in_reference_models, x, non_member_loader):\n    \"\"\"\n    Compute the likelihood ratio LR(x) = P(Model | x) / P(x).\n\n    Args:\n        reference_models: List of OUT models (trained on non-members).\n        in_reference_models: List of IN models (trained with x).\n        x: Target data point (dictionary with 'input' and 'label').\n        non_member_loader: DataLoader providing access to non-member data.\n    \n    Returns:\n        likelihood_ratio: The likelihood ratio LR(x).\n    \"\"\"\n    P_x = compute_P_x(reference_models, x, non_member_loader)\n    P_Model_given_x = compute_P_Model_given_x(in_reference_models, x)\n    likelihood_ratio = P_Model_given_x / (P_x + 1e-10)  # Add small value to avoid division by zero\n    return likelihood_ratio\n\n\n\n# Perform RMIA\ndef rmi_attack(target_model, in_reference_models, out_reference_models, member_loader, non_member_loader):\n    \"\"\"\n    Perform the Robust Membership Inference Attack (RMIA).\n\n    Args:\n        target_model: The target model to attack.\n        in_reference_models: List of IN reference models (trained with members).\n        out_reference_models: List of OUT reference models (trained without members).\n        member_loader: DataLoader providing access to member data.\n        non_member_loader: DataLoader providing access to non-member data.\n\n    Returns:\n        member_scores: Likelihood ratio scores for member samples.\n        non_member_scores: Likelihood ratio scores for non-member samples.\n    \"\"\"\n    member_scores = []\n    non_member_scores = []\n\n    # Evaluate member samples\n    print(\"Computing Likelihood Ratios for Members...\")\n    for inputs, labels in member_loader:\n        for i in range(len(inputs)):\n            x = {\"input\": inputs[i], \"label\": labels[i].item()}\n            lrt_score = compute_likelihood_ratio(out_reference_models, in_reference_models, x, non_member_loader)\n            member_scores.append(lrt_score)\n\n    # Evaluate non-member samples\n    print(\"Computing Likelihood Ratios for Non-Members...\")\n    for inputs, labels in non_member_loader:\n        for i in range(len(inputs)):\n            x = {\"input\": inputs[i], \"label\": labels[i].item()}\n            lrt_score = compute_likelihood_ratio(out_reference_models, in_reference_models, x, non_member_loader)\n            non_member_scores.append(lrt_score)\n\n    return member_scores, non_member_scores\n\n\n# Evaluate Attack\ndef evaluate_attack(member_scores, non_member_scores, threshold):\n    \"\"\"\n    Evaluate the effectiveness of the RMIA.\n\n    Args:\n        member_scores: Likelihood ratio scores for member samples.\n        non_member_scores: Likelihood ratio scores for non-member samples.\n        threshold: Threshold for deciding membership.\n\n    Returns:\n        accuracy, precision, recall, F1-score for the attack.\n    \"\"\"\n    # True labels: 1 for members, 0 for non-members\n    true_labels = [1] * len(member_scores) + [0] * len(non_member_scores)\n    predicted_labels = [1 if score > threshold else 0 for score in member_scores + non_member_scores]\n\n    # Compute metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels)\n    recall = recall_score(true_labels, predicted_labels)\n    f1 = f1_score(true_labels, predicted_labels)\n\n    print(f\"Attack Results - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n          f\"Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n    return accuracy, precision, recall, f1","metadata":{"_uuid":"576560da-655e-4701-8619-0f46af425551","_cell_guid":"40130ef6-0769-4661-947e-a91b02840af2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline  \n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef compute_roc(member_scores, non_member_scores):\n    \"\"\"\n    Compute TPR, FPR, and thresholds for ROC curve.\n    \n    Args:\n        member_scores: Likelihood ratio scores for member samples.\n        non_member_scores: Likelihood ratio scores for non-member samples.\n    \n    Returns:\n        fpr: False Positive Rate.\n        tpr: True Positive Rate.\n        thresholds: Thresholds used for the ROC curve.\n        roc_auc: Area Under the ROC Curve.\n    \"\"\"\n    # True labels: 1 for members, 0 for non-members\n    true_labels = [1] * len(member_scores) + [0] * len(non_member_scores)\n    scores = member_scores + non_member_scores\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(true_labels, scores)\n    roc_auc = auc(fpr, tpr)\n    \n    return fpr, tpr, thresholds, roc_auc\n\ndef plot_roc_curve(reference_models_list, member_loader, non_member_loader, target_model):\n    \"\"\"\n    Plot ROC curves as the number of reference models varies.\n\n    Args:\n        reference_models_list: List of lists, each containing a varying number of reference models.\n        member_loader: DataLoader for member samples.\n        non_member_loader: DataLoader for non-member samples.\n        target_model: Target model to attack.\n    \"\"\"\n    if not isinstance(reference_models_list, list) or not all(isinstance(models, list) for models in reference_models_list):\n        raise ValueError(\"reference_models_list must be a list of lists of models\")\n\n    plt.figure(figsize=(10, 7))\n\n    for i, reference_models in enumerate(reference_models_list):\n        if not reference_models:\n            raise ValueError(f\"Reference models for index {i} are empty or invalid.\")\n        \n        print(f\"Evaluating for {len(reference_models)} reference models...\")\n        member_scores, non_member_scores = rmi_attack(\n            target_model,\n            reference_models[:len(reference_models)//2],  # IN reference models\n            reference_models[len(reference_models)//2:],  # OUT reference models\n            member_loader,\n            non_member_loader\n        )\n\n        # Compute ROC\n        fpr, tpr, thresholds, roc_auc = compute_roc(member_scores, non_member_scores)\n\n        # Plot ROC curve\n        plt.plot(fpr, tpr, label=f\"{len(reference_models)} Models (AUC = {roc_auc:.4f})\")\n\n    plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal line\n    plt.title(\"ROC Curves for RMIA with Varying Reference Models\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc=\"lower right\")\n    plt.grid()\n    plt.show()","metadata":{"_uuid":"1179eeeb-a42c-4c2c-b81f-071b6e367a67","_cell_guid":"858b0344-4964-41c7-ac98-aead14e690ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_models = len(reference_models)\nin_reference_models = reference_models[:num_models // 2]\nout_reference_models = reference_models[num_models // 2:]\n\n\n# Perform Attack and Evaluate\nprint(\"Performing RMIA...\")\nmember_scores, non_member_scores = rmi_attack(\n    target_model, in_reference_models, out_reference_models, member_loader, non_member_loader\n)\n\n# Use a dynamic threshold (e.g., median of all scores)\nall_scores = member_scores + non_member_scores\nthreshold = np.median(all_scores)\n\n# Evaluate the attack\nevaluate_attack(member_scores, non_member_scores, threshold)","metadata":{"_uuid":"57df8f29-c31e-4948-b6a3-6cddcf6a538b","_cell_guid":"4877040b-9751-4246-b0fc-dac1670b9504","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(reference_models)","metadata":{"_uuid":"dd2da171-2ebd-49ad-80fc-d0c33e10fbd7","_cell_guid":"8b47b8cd-face-400f-adc8-00825e68ae30","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\nplot_roc_curve([reference_models], member_loader, non_member_loader, target_model)","metadata":{"_uuid":"e0c0cf28-5647-4a14-88d2-5b1e6a5f3a19","_cell_guid":"e54d2663-4f05-4a3a-aaa6-6d15cfdf74f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"32033140-feca-4227-92b1-ef30dc6595f2","_cell_guid":"07974685-f69a-4ba9-b7dd-dcc28e1db1bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}